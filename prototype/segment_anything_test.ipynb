{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGYUl5H0zvSH",
        "outputId": "51b8b6f3-dd34-47a2-e72a-17a922592a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-t454xm9_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-t454xm9_\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36587 sha256=421bb5637726ca2ce97909392380f1b5c12f6f824f8e4e33157c71d34b4eab3f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0hwi46p6/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMgCu_Tl0gh4",
        "outputId": "4e80c28e-15ac-4c9f-fd24-a79c18c0b198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5X2ZBjF5ZPp",
        "outputId": "ddcd62ad-b522-461c-f3ab-df113cf53842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-vhntsqs3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-vhntsqs3\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=3887fe6a0f7fde8182ae72c4ac4d17bf88c629e61927c3d95d46957244578f72\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0a58led8/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfXQjq_X0ove",
        "outputId": "997762b4-953d-4c3f-c1bd-56941f3b9424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q8LLcN91UDv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "from functools import lru_cache\n",
        "from random import randint\n",
        "from typing import Any, Callable, Dict, List, Tuple\n",
        "\n",
        "import clip\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"SAM\")\n",
        "CHECKPOINT_NAME = \"sam_vit_h_4b8939.pth\"\n",
        "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "MODEL_TYPE = \"default\"\n",
        "MAX_WIDTH = MAX_HEIGHT = 1024\n",
        "TOP_K_OBJ = 100\n",
        "THRESHOLD = 0.85\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def load_mask_generator() -> SamAutomaticMaskGenerator:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        os.makedirs(CHECKPOINT_PATH)\n",
        "    checkpoint = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
        "    if not os.path.exists(checkpoint):\n",
        "        urllib.request.urlretrieve(CHECKPOINT_URL, checkpoint)\n",
        "    sam = sam_model_registry[MODEL_TYPE](checkpoint=checkpoint).to(device)\n",
        "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "    return mask_generator\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def load_clip(\n",
        "    name: str = \"ViT-B/32\",\n",
        ") -> Tuple[torch.nn.Module, Callable[[PIL.Image.Image], torch.Tensor]]:\n",
        "    model, preprocess = clip.load(name, device=device)\n",
        "    return model.to(device), preprocess\n",
        "\n",
        "\n",
        "def adjust_image_size(image: np.ndarray) -> np.ndarray:\n",
        "    height, width = image.shape[:2]\n",
        "    if height > width:\n",
        "        if height > MAX_HEIGHT:\n",
        "            height, width = MAX_HEIGHT, int(MAX_HEIGHT / height * width)\n",
        "    else:\n",
        "        if width > MAX_WIDTH:\n",
        "            height, width = int(MAX_WIDTH / width * height), MAX_WIDTH\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    return image\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_score(crop: PIL.Image.Image, texts: List[str]) -> torch.Tensor:\n",
        "    model, preprocess = load_clip()\n",
        "    preprocessed = preprocess(crop).unsqueeze(0).to(device)\n",
        "    tokens = clip.tokenize(texts).to(device)\n",
        "    logits_per_image, _ = model(preprocessed, tokens)\n",
        "    similarity = logits_per_image.softmax(-1).cpu()\n",
        "    return similarity[0, 0]\n",
        "\n",
        "\n",
        "def crop_image(image: np.ndarray, mask: Dict[str, Any]) -> PIL.Image.Image:\n",
        "    x, y, w, h = mask[\"bbox\"]\n",
        "    masked = image * np.expand_dims(mask[\"segmentation\"], -1)\n",
        "    crop = masked[y : y + h, x : x + w]\n",
        "    if h > w:\n",
        "        top, bottom, left, right = 0, 0, (h - w) // 2, (h - w) // 2\n",
        "    else:\n",
        "        top, bottom, left, right = (w - h) // 2, (w - h) // 2, 0, 0\n",
        "    # padding\n",
        "    crop = cv2.copyMakeBorder(\n",
        "        crop,\n",
        "        top,\n",
        "        bottom,\n",
        "        left,\n",
        "        right,\n",
        "        cv2.BORDER_CONSTANT,\n",
        "        value=(0, 0, 0),\n",
        "    )\n",
        "    crop = PIL.Image.fromarray(crop)\n",
        "    return crop\n",
        "\n",
        "\n",
        "def get_texts(query: str) -> List[str]:\n",
        "    return [f\"a picture of {query}\", \"a picture of background\"]\n",
        "\n",
        "\n",
        "def filter_masks(\n",
        "    image: np.ndarray,\n",
        "    masks: List[Dict[str, Any]],\n",
        "    predicted_iou_threshold: float,\n",
        "    stability_score_threshold: float,\n",
        "    query: str,\n",
        "    clip_threshold: float,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    filtered_masks: List[Dict[str, Any]] = []\n",
        "\n",
        "    for mask in sorted(masks, key=lambda mask: mask[\"area\"])[-TOP_K_OBJ:]:\n",
        "        if (\n",
        "            mask[\"predicted_iou\"] < predicted_iou_threshold\n",
        "            or mask[\"stability_score\"] < stability_score_threshold\n",
        "            or image.shape[:2] != mask[\"segmentation\"].shape[:2]\n",
        "            or query\n",
        "            and get_score(crop_image(image, mask), get_texts(query)) < clip_threshold\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        filtered_masks.append(mask)\n",
        "\n",
        "    return filtered_masks\n",
        "\n",
        "\n",
        "def remove_small_segments(segmentation: np.ndarray) -> np.ndarray:\n",
        "    # ブール配列を整数型に変換（OpenCVの関数はブール型を直接扱えないため）\n",
        "    segmentation_int = segmentation.astype(np.uint8)  # Trueを1に、Falseを0に変換\n",
        "\n",
        "    # すべての連結成分を見つけ、ラベル付けする\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(segmentation_int)\n",
        "\n",
        "    # 最大の連結成分（背景を除く）のラベルを見つける\n",
        "    # 面積はstatsの5番目の列に格納されています（index=4）\n",
        "    # 背景の成分（ラベル0）を除外して最大のものを見つける\n",
        "    largest_label = 1 + np.argmax(stats[1:, 4])  # 背景を除く最大領域\n",
        "    # 最大の連結成分のみを保持\n",
        "    cleaned_segmentation = (labels == largest_label)\n",
        "\n",
        "    return cleaned_segmentation\n",
        "\n",
        "def remove_contained_masks(masks: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    # マスクが他のマスクに完全に含まれているかどうかをチェック\n",
        "    remaining_masks = []\n",
        "    for i, mask_i in enumerate(masks):\n",
        "        fully_contained = False\n",
        "        for j, mask_j in enumerate(masks):\n",
        "            if i != j and np.all(mask_i[\"segmentation\"] <= mask_j[\"segmentation\"]):\n",
        "                fully_contained = True\n",
        "                break\n",
        "        if not fully_contained:\n",
        "            remaining_masks.append(mask_i)\n",
        "    return remaining_masks\n",
        "\n",
        "def remove_overlapping_masks(masks: List[np.ndarray], overlap_threshold: float = 0.8) -> List[np.ndarray]:\n",
        "    # マスクが他のマスクと大きく重複しているかどうかをチェックし、重複している場合は小さい方を削除\n",
        "    remaining_masks = []\n",
        "    removed_indices = set()  # 削除されたマスクのインデックスを保持\n",
        "\n",
        "    for i, mask_i in enumerate(masks):\n",
        "        if i in removed_indices:\n",
        "            continue  # すでに削除されているマスクはスキップ\n",
        "\n",
        "        for j, mask_j in enumerate(masks):\n",
        "            if i != j and j not in removed_indices:\n",
        "                # 両マスク間の重複領域を計算\n",
        "                intersection = np.logical_and(mask_i[\"segmentation\"], mask_j[\"segmentation\"])\n",
        "                intersection_area = np.sum(intersection)\n",
        "\n",
        "                # 小さい方のマスクの面積を計算\n",
        "                area_i = np.sum(mask_i[\"segmentation\"])\n",
        "                area_j = np.sum(mask_j[\"segmentation\"])\n",
        "                min_area = min(area_i, area_j)\n",
        "\n",
        "                # 重複領域が小さい方のマスクの面積の特定の割合以上なら、小さい方のマスクを削除\n",
        "                if intersection_area / min_area > overlap_threshold:\n",
        "                    if area_i < area_j:\n",
        "                        removed_indices.add(i)\n",
        "                        break  # 現在のマスクiを削除し、次のマスクに進む\n",
        "                    else:\n",
        "                        removed_indices.add(j)\n",
        "                        # マスクjを削除しても、マスクiの処理は続ける\n",
        "\n",
        "    # 削除されていないマスクのみを保持\n",
        "    for i, mask in enumerate(masks):\n",
        "        if i not in removed_indices:\n",
        "            remaining_masks.append(mask)\n",
        "\n",
        "    return remaining_masks\n",
        "\n",
        "\n",
        "def draw_masks(\n",
        "    image: np.ndarray, masks: List[np.ndarray], alpha: float = 0.7\n",
        ") -> np.ndarray:\n",
        "    masks = remove_overlapping_masks(masks)\n",
        "    surfaces = []\n",
        "    transparent_mask = np.zeros_like(image)\n",
        "\n",
        "    for mask in masks:\n",
        "        segmentation = remove_small_segments(mask[\"segmentation\"])\n",
        "        area = np.sum(segmentation)\n",
        "\n",
        "        if mask[\"segmentation\"].size * 0.01 > area:\n",
        "          continue\n",
        "\n",
        "        color = [randint(127, 255) for _ in range(3)]\n",
        "\n",
        "        # draw mask overlay\n",
        "        colored_mask = np.expand_dims(segmentation, 0).repeat(3, axis=0)\n",
        "        colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
        "\n",
        "        # masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
        "        # image_overlay = masked.filled()\n",
        "        # image = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
        "\n",
        "        # draw contour\n",
        "        contours, _ = cv2.findContours(\n",
        "            np.uint8(segmentation), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "        cv2.drawContours(image, contours, -1, (0, 0, 255), 2)\n",
        "\n",
        "        for contour in contours:\n",
        "            # Calculate the perimeter of the contour\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            # Approximate the contour to a polygon\n",
        "            epsilon = 0.02 * perimeter  # 2% of the perimeter\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            if len(approx) == 4:\n",
        "              # Draw the approximated polygon (should be a quadrilateral if the shape is close to a rectangle)\n",
        "              cv2.drawContours(image, [approx], 0, (255, 0, 0), 2)\n",
        "              cv2.drawContours(transparent_mask, [approx], 0, (255, 0, 0), -1)\n",
        "\n",
        "              surfaces.append(np.squeeze(approx, axis=1))\n",
        "\n",
        "    image = cv2.addWeighted(transparent_mask, 1 - alpha, image, alpha, 0)\n",
        "\n",
        "    return image, surfaces\n",
        "\n",
        "def crop_and_affine_transform_quadrilateral(original_image: np.ndarray, src_pts: np.ndarray) -> np.ndarray:\n",
        "    # src_ptsが平行四辺形であると仮定して、アフィン変換を適用\n",
        "    # 変換後の点を定義 (左上、右上、左下の順)\n",
        "    width_a = np.sqrt(((src_pts[0][0][0] - src_pts[1][0][0]) ** 2) + ((src_pts[0][0][1] - src_pts[1][0][1]) ** 2))\n",
        "    width_b = np.sqrt(((src_pts[2][0][0] - src_pts[3][0][0]) ** 2) + ((src_pts[2][0][1] - src_pts[3][0][1]) ** 2))\n",
        "    height_a = np.sqrt(((src_pts[0][0][0] - src_pts[3][0][0]) ** 2) + ((src_pts[0][0][1] - src_pts[3][0][1]) ** 2))\n",
        "    height_b = np.sqrt(((src_pts[1][0][0] - src_pts[2][0][0]) ** 2) + ((src_pts[1][0][1] - src_pts[2][0][1]) ** 2))\n",
        "    max_width = max(int(width_a), int(width_b))\n",
        "    max_height = max(int(height_a), int(height_b))\n",
        "    dst_pts = np.array([[0, 0], [max_width - 1, 0], [0, max_height - 1]], dtype='float32')\n",
        "\n",
        "    # 3つの点からアフィン変換行列を計算\n",
        "    M = cv2.getAffineTransform(np.float32(src_pts[:3]), dst_pts)\n",
        "\n",
        "    # アフィン変換を適用して画像を変換\n",
        "    transformed = cv2.warpAffine(original_image, M, (max_width, max_height))\n",
        "\n",
        "    return transformed\n",
        "\n",
        "def crop_test(img, points):\n",
        "    points = sorted(points, key=lambda x:x[1])  # yが小さいもの順に並び替え。\n",
        "    top = sorted(points[:2], key=lambda x:x[0])  # 前半二つは四角形の上。xで並び替えると左右も分かる。\n",
        "    bottom = sorted(points[2:], key=lambda x:x[0], reverse=True)  # 後半二つは四角形の下。同じくxで並び替え。\n",
        "    points = np.array(top + bottom, dtype='float32')  # 分離した二つを再結合。\n",
        "\n",
        "    width = max(np.sqrt(((points[0][0]-points[2][0])**2)*2), np.sqrt(((points[1][0]-points[3][0])**2)*2))\n",
        "    height = max(np.sqrt(((points[0][1]-points[2][1])**2)*2), np.sqrt(((points[1][1]-points[3][1])**2)*2))\n",
        "\n",
        "    dst = np.array([\n",
        "            np.array([0, 0]),\n",
        "            np.array([width-1, 0]),\n",
        "            np.array([width-1, height-1]),\n",
        "            np.array([0, height-1]),\n",
        "            ], np.float32)\n",
        "\n",
        "    trans = cv2.getPerspectiveTransform(points, dst)  # 変換前の座標と変換後の座標の対応を渡すと、透視変換行列を作ってくれる。\n",
        "    return cv2.warpPerspective(img, trans, (int(width), int(height)))\n",
        "\n",
        "def normalize_surface(surface, image_width, image_height):\n",
        "    # surfaceの各頂点を正規化（0から1の範囲に変換）\n",
        "    normalized_surface = np.zeros_like(surface, dtype=np.float32)\n",
        "    normalized_surface[:, 0] = surface[:, 0] / image_width  # x座標を正規化\n",
        "    normalized_surface[:, 1] = surface[:, 1] / image_height  # y座標を正規化\n",
        "\n",
        "    return normalized_surface\n",
        "\n",
        "def denormalize_approx(surface, image_width, image_height):\n",
        "    denormalized_approx = np.zeros_like(surface, dtype=np.int32)\n",
        "    denormalized_approx[:, 0] = np.round(surface[:, 0] * image_width).astype(np.int32)  # x座標を元に戻す\n",
        "    denormalized_approx[:, 1] = np.round(surface[:, 1] * image_height).astype(np.int32)  # y座標を元に戻す\n",
        "\n",
        "    return denormalized_approx\n",
        "\n",
        "def segment_frame(predicted_iou_threshold: float,\n",
        "    stability_score_threshold: float,\n",
        "    clip_threshold: float,\n",
        "    frame: str,\n",
        "    query: str,\n",
        "    mask_generator,\n",
        "):\n",
        "    # mask_generator = load_mask_generator()\n",
        "    # ori_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # reduce the size to save gpu memory\n",
        "    image = adjust_image_size(frame)\n",
        "    try:\n",
        "      masks = mask_generator.generate(image)\n",
        "      masks = filter_masks(\n",
        "          image,\n",
        "          masks,\n",
        "          predicted_iou_threshold,\n",
        "          stability_score_threshold,\n",
        "          query,\n",
        "          clip_threshold,\n",
        "      )\n",
        "      masked_image, _ = draw_masks(image, masks)\n",
        "      # masked_image = PIL.Image.fromarray(masked_image)\n",
        "      return masked_image\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(\"error occur\")\n",
        "      return image\n",
        "\n",
        "def segment(\n",
        "    predicted_iou_threshold: float,\n",
        "    stability_score_threshold: float,\n",
        "    clip_threshold: float,\n",
        "    image_path: str,\n",
        "    query: str,\n",
        ") -> PIL.ImageFile.ImageFile:\n",
        "    mask_generator = load_mask_generator()\n",
        "    ori_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    ori_image = cv2.cvtColor(ori_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # reduce the size to save gpu memory\n",
        "    image = adjust_image_size(ori_image)\n",
        "    masks = mask_generator.generate(image)\n",
        "    masks = filter_masks(\n",
        "        image,\n",
        "        masks,\n",
        "        predicted_iou_threshold,\n",
        "        stability_score_threshold,\n",
        "        query,\n",
        "        clip_threshold,\n",
        "    )\n",
        "    masked_image, surfaces = draw_masks(image, masks)\n",
        "\n",
        "    # cropped_images = []\n",
        "    normalized_surfaces = []\n",
        "    for surface in surfaces:\n",
        "      height, width, _ = image.shape\n",
        "      nor_surface = normalize_surface(surface, width, height)\n",
        "      normalized_surfaces.append(nor_surface)\n",
        "    #   ori_h, ori_w, _ = ori_image.shape\n",
        "    #   sca_surface = denormalize_approx(surface, ori_w, ori_h)\n",
        "\n",
        "      # cropped_img = crop_test(ori_image, sca_surface)\n",
        "      # cropped_images.append(cropped_img)\n",
        "\n",
        "    masked_image = PIL.Image.fromarray(masked_image)\n",
        "    return masked_image, surfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "VRFqO9if5r1p",
        "outputId": "e790960f-1ad4-4f81-99cc-bbdf30bd8911"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/drive/MyDrive/未踏/prototype/demo.jpg\"\n",
        "\n",
        "image, surfaces = segment(0.8, 0.8, 0.96, filename, \"screen\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7nOQ1NF53qx"
      },
      "outputs": [],
      "source": [
        "ori_image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
        "ori_image = cv2.cvtColor(ori_image, cv2.COLOR_BGR2RGB)\n",
        "surface = surfaces[0]\n",
        "ori_h, ori_w, _ = ori_image.shape\n",
        "ori_image = cv2.resize(ori_image, (int(ori_w * 0.8), int(ori_h * 0.8)))\n",
        "ori_h, ori_w, _ = ori_image.shape\n",
        "sca_surface = denormalize_approx(surface, ori_w, ori_h)\n",
        "cropped_img = crop_test(ori_image, sca_surface)\n",
        "PIL.Image.fromarray(cropped_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "-uTL-T10IVry",
        "outputId": "6e680342-47ea-45a4-cb89-adfecc2fa221"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/drive/MyDrive/未踏/prototype/demo2.jpg\"\n",
        "\n",
        "image, surfaces = segment(0.8, 0.8, 0.96, filename, \"display\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBS_IFy-Dm6G"
      },
      "source": [
        "### 動画を処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfU3hJGnc9WA",
        "outputId": "76314411-7c9a-4da0-ee56-21dcdc9485ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 26.66666666666664/100 [12:47<32:07, 26.28s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slice indices must be integers or None or have an __index__ method\n",
            "error occur\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 31.999999999999954/100 [15:14<29:24, 25.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slice indices must be integers or None or have an __index__ method\n",
            "error occur\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 99.99999999999966/100 [48:21<00:00, 29.01s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/未踏/prototype/demo.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# 動画のフレームレートを取得\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "_, first_frame = cap.read()\n",
        "\n",
        "first_frame = adjust_image_size(first_frame.copy())\n",
        "height, width = first_frame.shape[:2]\n",
        "\n",
        "# 10秒間に相当するフレーム数を計算\n",
        "frames_to_process = int(10 * fps)\n",
        "\n",
        "# 出力用の動画ファイルを準備\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "# 処理するフレーム数をカウント\n",
        "frame_count = 0\n",
        "\n",
        "mask_generator = load_mask_generator()\n",
        "\n",
        "for _ in range(int(5 * fps)):\n",
        "  cap.read()\n",
        "\n",
        "with tqdm(total=100) as pbar:\n",
        "  while(cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if not ret or frame_count >= frames_to_process:\n",
        "          break\n",
        "\n",
        "      # フレームごとに物体検出を実行\n",
        "      detected_frame = segment_frame(0.8, 0.8, 0.96, frame.copy(), \"display\", mask_generator)\n",
        "      # detected_frame = segment_frame(0.8, 0.8, 0.96, frame.copy(), \"display\")\n",
        "\n",
        "      # 出力用の動画ファイルに書き込む\n",
        "      out.write(detected_frame)\n",
        "\n",
        "      pbar.update(100/frames_to_process)\n",
        "      frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FdmHvTRGB2o",
        "outputId": "fcce9460-3638-4b77-c0e6-eb13db2e2a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "6220800 1769472\n"
          ]
        }
      ],
      "source": [
        "print(frame.__class__)\n",
        "print(detected_frame.__class__)\n",
        "print(frame.size, detected_frame.size)\n",
        "\n",
        "masked_image = PIL.Image.fromarray(detected_frame)\n",
        "\n",
        "# masked_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzqRMLP3ad5e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
